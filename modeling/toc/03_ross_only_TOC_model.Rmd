---
title: "Testing TOC preds"
author: "Matthew Ross & Sam Struthers-ROSSyndicate"
date: "2025-08-01"
output: html_document
---

# Package load

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
source("src/setup_libraries.R")

#these are just packages for testing within this script
library(gridExtra)
library(grid)
library(Ckmeans.1d.dp)
library(DiagrammeR)
library(Metrics)
library(caret)
library(glue)
set.seed(123)

source("src/apply_sensor_transformations_toc.R")
```

# Create scaling based on all sensor data

```{r}
# Read in all data to create scaling parameters
wide_data_file <- list.files("data/upper_clp_dss/sensor/prepped/", pattern = "all_sensor_data_.*\\.parquet", full.names = TRUE)%>%
  tail(1)


all_sensor_data_wide <- read_parquet(wide_data_file)

# Use the complete sensor dataset for scaling parameters (assuming sensor has all necessary columns already)
complete_sensor_data <- all_sensor_data_wide %>%
  apply_sensor_transformations_toc(., dt_col = "DT_hourly_round")


# Save the original min/max values for each numeric column, removing NAs
scaling_params <- complete_sensor_data %>%
  select(where(is.numeric)) %>%
  summarise(across(
    everything(),
    list(min = ~min(.x, na.rm = TRUE), max = ~max(.x, na.rm = TRUE)),
    .names = "{.col}_{.fn}"
  ))


# Save these parameters
#saveRDS(scaling_params, "data/upper_clp_dss/modeling/scaling_parameters_20251007.rds")
# Load saved scaling parameters
#scaling_params <- readRDS("data/upper_clp_dss/modeling/scaling_parameters_20250820.rds")

# Function to apply the same scaling to new data
apply_training_scale <- function(new_data, scaling_params) {
  numeric_cols <- names(select(new_data, where(is.numeric)))
  
  scaled_data <- new_data
  
  for (col in numeric_cols) {
    min_val <- scaling_params[[paste0(col, "_min")]]
    max_val <- scaling_params[[paste0(col, "_max")]]
    
    # Apply same min-max scaling as training data
    scaled_data[[col]] <- (new_data[[col]] - min_val) / (max_val - min_val)
  }
  
  return(scaled_data)
}
```


# Read in Data

```{r}
matchup_dataset <- list.files("data/upper_clp_dss/modeling/", pattern = "parsed_data_ROSS_2025", full.names = TRUE)%>%
  tail(1)

# Read in all matches data
all_matches <- read_csv(file = matchup_dataset, show_col_types = F) %>%
  apply_sensor_transformations_toc(dt_col = "sensor_datetime")%>%
  #remove NAs
  filter(!is.na(TOC))%>%
  mutate(clean_sensor_datetime = parse_date_time(sensor_datetime, orders = c("%Y-%m-%d %H:%M:%S", "%m/%d/%y %H:%M","%Y-%m-%d" )))%>%
  mutate(sensor_datetime = force_tz(clean_sensor_datetime, tzone = 'MST'), 
         month = lubridate::month(sensor_datetime))

id_cols <- c("Date", "id", "sensor_datetime","lab_datetime",  "collector", "data_avail")

# Apply scaling to dataset
all_matches_norm <- all_matches %>%
  #Only keep columns that would be in the complete sensor dataset
  select(any_of(id_cols), any_of(names(complete_sensor_data)))%>%
  #normalize data based on full sensor set from 01_sensor_data_prep.rmd and the chunk above
  apply_training_scale(., scaling_params)%>%
  #add back in TOC (non normalized)
  mutate(TOC = all_matches$TOC)%>%
  na.omit()

```

# Set aside testing dataset & select features

```{r}

target  = "TOC"
features <- setdiff(colnames(all_matches_norm), c(id_cols, target))

all_matches_trimmed <- all_matches_norm%>%
  filter(!(id == "cbri" & between(sensor_datetime, "2024-10-08", "2024-10-10")))%>% #exclude a cbri sample taken during release
  filter(!(id == "pman" & between(sensor_datetime, "2025-05-28", "2025-06-01")))%>% #exclude a pman sample with bad sensor data
  filter( id %nin% c("springcreek", "boxcreek"))


testing <- all_matches_trimmed %>%
  filter((year(sensor_datetime) == 2025 & id %in% c("chd", "sfm", "pbr", "pman", "pbd")))


train_val <- all_matches_trimmed %>%
  anti_join(testing)%>%
  select(id, any_of(features),TOC, sensor_datetime, collector)

```


# Train all models across CV folds

## Define and visualize folds

```{r}
# Define validation sets for each fold
val_set_1 <- c( "sfm","chd","lbea","riverbluffs" )

val_set_2 <- c( "pbd","cbri", "penn", "archery_virridy" )

val_set_3 <- c("pfal","udall","joei","archery","elc","salyer","riverbend")


# get counts per site in train_val
train_val %>%
  group_by(id) %>%
  summarise(count = n(), .groups = "drop")%>%
  arrange(desc(count))


# Function to create plots for a given fold
plot_fold <- function(fold_id, train_val) {
  val_set_name <- paste0("val_set_", fold_id)
  val_set <- train_val %>% filter(id %in% get(val_set_name))
  train_set <- train_val %>% filter(id %nin% get(val_set_name))
  
  prop_val <- round(nrow(val_set)/nrow(train_val), 3) * 100
  train_med <- round(median(train_set$TOC, na.rm = TRUE), 1)
  val_med <- round(median(val_set$TOC, na.rm = TRUE), 1)
  
  # Histogram
  hist <- ggplot(train_set, aes(x = TOC)) +
    geom_histogram(bins = 30, fill = "grey") +
    geom_histogram(data = val_set, aes(x = TOC), bins = 30, fill = "#E70870", alpha = 0.5) +
    labs(
      title = paste0("Fold Set ", fold_id, ": ", nrow(val_set), " samples (", prop_val, "%) in validation set"),
      subtitle = paste0("Validation Sites: ", paste(unique(val_set$id), collapse = ", ")),
      x = "TOC (mg/L)"
    ) +
    theme_minimal()
  
  # Boxplot
  box <- ggplot(
    train_set %>% mutate(set = "Train") %>% 
      bind_rows(val_set %>% mutate(set = "Validation")) %>% 
      mutate(set = factor(set, levels = c("Train", "Validation"))),
    aes(x = set, y = TOC, fill = set)
  ) +
    geom_boxplot() +
    scale_fill_manual(values = c("Train" = "#256BF5", "Validation" = "#E70870")) +
    labs(
      title = paste0("Train Median TOC: ",train_med, "\nVal Median TOC: ", val_med),
      x = "",
      y = "TOC (mg/L)",
      fill = "TV Group",
    ) +
    theme_minimal()
  
  # Combine plots
  ggarrange(hist, box, nrow = 1)
}

# Create a list to store all fold plots
fold_plots <- list()
for (i in 1:3) {
  fold_plots[[i]] <- plot_fold(i, train_val)
}
ggarrange(plotlist = fold_plots, ncol = 1)



```

## Visualize TOC distribution in training/validation vs testing sets

```{r}


ggplot(train_val, aes(x = TOC, fill = collector)) +
  geom_histogram(bins = 30) +
  geom_histogram(data = testing, aes(x = TOC), bins = 30, fill = "#E70870", alpha = 0.5) +
  labs(
    title = paste0("Training/Validation Set: ", nrow(train_val), " samples                Testing Set: ", nrow(testing), " samples"),
    subtitle = paste0("Training TOC Range: ", round(min(train_val$TOC, na.rm = TRUE),2), " - ", round(max(train_val$TOC, na.rm = TRUE),2) , " mg/L (Median: ", round(median(train_val$TOC), 2), ")\nTesting TOC Range: ", round(min(testing$TOC, na.rm = TRUE),2), " - ", round(max(testing$TOC, na.rm = TRUE),2), " mg/L (Median: ", round(median(testing$TOC), 2),")"),
    x = "TOC (mg/L)"
  ) +
  theme_minimal()


```


## Visualize TOC weighting approach

```{r}

# Example data
data <- tibble(toc = seq.int(0, 10, 0.1)) %>%
  # inverse weight: higher TOC -> lower weight
  mutate(weight  = ifelse(toc <= 4, 
                          1,                  # flat weight for low TOC
                          1 + 2 *log1p(toc - 4))  # smooth increase after 2.5
  )  # simple inverse scaling

# Plotting
ggplot(data, aes(x = toc)) +
  geom_line(aes(y = weight), color = "blue") +
  geom_hline(yintercept = 1, linetype = "dashed", color = "red") +
  labs( x = "TOC (mg/L)", y = "Weight", title = "TOC Model Weighting Approach (red line = 1)") +
  theme_minimal()+
  ylim(0,7.5)


```



# Grid search and train models
Create 3 seperate folds and 3 models, with one fold used for validation on each model. 

```{r}
source("src/xgboost_feature_tuning.R")

folds <- tibble(
  fold = 1:3,
  val_ids = list(val_set_1, val_set_2, val_set_3)
)

toc_high_weights <-  function(df) {
  ifelse(df$TOC <= 4,1,                  # flat weight for low TOC
         1 + 3 *log1p(df$TOC - 4))  # grows slowly, no cap needed
}

features_test <- c("FDOMc", "Sensor_Cond","Chl_a", "Temp", # raw sensor values
               "f_x_turb", "f_sc", "temp_f", #product features
               "fdom_corr_max_6hr", "temp_max_24hr", "turb_median_12hr",#lagged features 
               "sin_doy" #seasonal
              )
            

# create feature grid (6,7, 8 feature combinations)
feature_grid <- map_dfr(6:8, ~ {
  combn(features_test, .x, simplify = FALSE) %>% 
    map(~ tibble(features = list(.x)))
})

    default_hyper_params <- tibble(
      nrounds = 10000,
      max_depth = 4,
      eta =  0.1,
      gamma = 0.8,
      colsample_bytree = 0.8,
      min_child_weight = 2,
      subsample = 0.8, 
      alpha = 0, 
      lambda = 1
    )


model <- xgboost_feature_tuning(
  data = train_val %>% select(TOC, id, any_of(features)), 
  weights = toc_high_weights,
  hyper_params = default_hyper_params,
  feature_grid = feature_grid,
  target_col = "TOC",
  site_col = "id",
  fold_ids =  folds, 
  units = "mg/L", 
  plot_dir = "data/upper_clp_dss/figures/toc_modeling/feature_tuning/"
)

#print out features for each model
for(i in 1:length(model)){
  cat(paste0("Fold ", i, " features: ", paste(model[[i]][["model"]][["feature_names"]], collapse = ", "), "\n"))
}

```

## Tune hyperparameters
```{r}
source("src/xgboost_hyperparameter_tuning.R")

folds <- tibble(
  fold = 1:3,
  val_ids = list(val_set_1, val_set_2, val_set_3)
)

toc_high_weights <-  function(df) {
  ifelse(df$TOC <= 4,1,                  # flat weight for low TOC
         1 + 2 *log1p(df$TOC - 4))  # grows slowly, no cap needed
}

features <- c("FDOMc", "Sensor_Cond",  # raw sensor values
               "f_x_turb", "f_sc", #product features
               "fdom_corr_max_6hr",  #lagged features
              "sin_doy") #seasonal features

# Reduce tune grid for reruns (ie. General best params)
tune_grid = expand.grid(
       nrounds = 10000,
       max_depth = c(2, 3, 4),
       eta = c( 0.01, 0.1),
       gamma = c(0.8),
       lambda = c(1,10),
       alpha = c(0, 1),
       colsample_bytree = c(0.5,0.8),
       min_child_weight = c(2, 4, 6),
       subsample = c(0.5,0.8)
     )

full_model <- xgboost_hyperparameter_tuning(
  data = train_val %>% select(TOC, id, any_of(features)), 
  weights = toc_high_weights,
  tune_grid = tune_grid,
  target_col = "TOC",
  site_col = "id",
  fold_ids =  folds, 
  units = "mg/L")

```
## Look at feature importance for each fold

```{r}
importance_list <- lapply(seq_along(full_model), function(i) {
  fold_model <- full_model[[i]]$model
  features <- fold_model$feature_names
  importance_matrix <- xgb.importance(feature_names = features, model = fold_model)
  importance_df <- as.data.frame(importance_matrix)
  importance_df$Fold <- paste("Fold", i)
  return(importance_df)
})

print(importance_list)
```



## Save files (smaller)
```{r}

small_models <- lapply(full_model, function(x) {
  m <- x$model   # extract model (xgboost object)
  
  #remove large unnecessary elements 
  if ("trainingData" %in% names(m)) m$trainingData <- NULL
  if ("call" %in% names(m)) m$call <- NULL
  if ("terms" %in% names(m)) m$terms <- NULL
  
  return(m)
})
saveRDS(small_models, file = paste0("data/upper_clp_dss/modeling/model_splits/ross_only_toc_xgboost_models_light_",Sys.Date(),".rds"))

```


# Read in Model
```{r}

most_recent_file <- list.files("data/upper_clp_dss/modeling/model_splits", pattern = "ross_only_toc_xgboost_models_light_.*\\.rds", full.names = TRUE) %>%
  tail(1)
model <- readRDS(most_recent_file)
```

## Look at feature importance for each fold

```{r}
importance_list <- lapply(seq_along(model), function(i) {
  fold_model <- model[[i]]
  features <- fold_model$feature_names
  importance_matrix <- xgb.importance(feature_names = features, model = fold_model)
  importance_df <- as.data.frame(importance_matrix)
  importance_df$Fold <- paste("Fold", i)
  return(importance_df)
})

print(importance_list)
```
## Look at shapley importance

```{r}
library(SHAPforxgboost)
# Function to plot SHAP values for a given fold
plot_shap <- function(fold_id, train_val_df, fold_model, top_n = 3) {
  # Predictions on validation/training set
  val_set_name <- paste0("val_set_", fold_id)
  val_data <- train_val_df %>% filter(id %in% get(val_set_name))
  train_data <- train_val_df %>% filter(!(id %in% get(val_set_name)))
  
  dtrain <- as.matrix(train_data[, features])
  
  shap_values <- shap.values(xgb_model = fold_model, X_train = dtrain)
  
  # Long-format SHAP values
  shap_long <- shap.prep(
    shap_contrib = shap_values$shap_score,
    X_train = as.matrix(train_data[, features])
  )
  
  # --- 1. SHAP summary plot (global view)
  return(shap.plot.summary(shap_long))

}

plots <- map(1:3, ~ plot_shap(
  fold_id = .x,
  fold_model = model[[.x]],
  train_val_df = train_val,
  top_n = 3
))

ggarrange(plotlist = plots, ncol = 3)

```



## Create plots of training + validation

```{r}

# Function to create plots for a given fold
plot_fold_performance <- function(fold_id, train_val_df, fold_model, target_col = "TOC", units = "mg/L") {
  # Predictions on validation/training set
  val_set_name <- paste0("val_set_", fold_id)
  val_data <- train_val_df %>% filter(id %in% get(val_set_name))
  train_data <- train_val_df %>% filter(id %nin% get(val_set_name))
  target_pred_col <- paste0(target_col, "_guess")
  
  features <- fold_model$feature_names
  
  #convert train_val sets to matrix for prediction
  val_matrix <- val_data[, features]%>%
    mutate(across(everything(), as.numeric)) %>%
    as.matrix()
  train_matrix <- train_data[, features]%>%
    mutate(across(everything(), as.numeric)) %>%
    as.matrix()
  
  #make predictions
  val_data[[target_pred_col]] <-  predict(fold_model, val_matrix, iterationrange = c(1, fold_model$best_iteration))
  val_data$group <-  "Validation"
  train_data[[target_pred_col]] <-  predict(fold_model, train_matrix, iterationrange = c(1, fold_model$best_iteration))
  train_data$group <-  "Train"
  
  # Calculate RMSE/MAE for annotation
  train_rmse <- rmse(train_data[[target_col]],train_data[[target_pred_col]]) %>% round(3)
  val_rmse <- rmse(val_data[[target_col]],val_data[[target_pred_col]]) %>% round(3)
  train_mae <- mae(train_data[[target_col]], train_data[[target_pred_col]]) %>% round(3)
  val_mae <- mae(val_data[[target_col]], val_data[[target_pred_col]]) %>% round(3)
  
  # Create performance plot on Train/val
  # Combine train and val sets for plotting
  data <- bind_rows(train_data, val_data)
  # get plot bounds
  min_val <- min(data[[target_col]], na.rm = TRUE)
  max_val <- max(data[[target_col]], na.rm = TRUE)
  
  plot_range <- max_val - min_val
  box_width <- plot_range * 0.5
  box_height <- plot_range * 0.2
  box_xmin <- min_val + plot_range * 0.02
  box_xmax <- box_xmin + box_width
  box_ymax <- max_val - plot_range * 0.02
  box_ymin <- box_ymax - box_height
  text_x <- (box_xmin + box_xmax) / 2
  text_y1 <- box_ymax - box_height * 0.15
  text_y2 <- box_ymax - box_height * 0.5
  text_y3 <- box_ymax - box_height * 0.85
  
  train_val_plot <- ggplot(data, aes(x = .data[[target_col]], y = .data[[target_pred_col]], color = group))+ #shape = collector)) +
    geom_abline(intercept = 0, slope = 1, linetype = "dashed", size = 1.2) +
    
    # Training points (all same shape)
    geom_point(
      data = filter(data, group == "Train"),
      #shape = 16,  # solid circle
      size = 4,
      alpha = 0.6) +
    geom_point( # Testing points (shape by id)
      data = filter(data, group == "Validation"),
      shape = 17,
      #aes(shape = id),
      size = 4,
      alpha = 0.9
    ) +
    annotate("rect", xmin = box_xmin, xmax = box_xmax, ymin = box_ymin, ymax = box_ymax,
             fill = "white", color = "black", alpha = 1, linewidth = 0.5) +
    annotate("text", x = text_x, y = text_y1, label = paste0("Training samples: n = ", (nrow(data) - nrow(val_data))),
             color = "black", size = 3) +
    annotate("text", x = text_x, y = text_y2, label = paste0("Validation samples: n = ", nrow(val_data)),
             color = "black", size = 3) +
    annotate("text", x = text_x, y = text_y3, label = paste0("Val RMSE: ", val_rmse,
                                                             " ",units,", MAE:", val_mae, " ", units),
             color = "black", fontface = 2, size = 3.5) +
    scale_color_manual(values = c("Train" = "#002EA3", "Validation" = "#FFCA3A")) +
    #scale_shape_manual(values = c("ROSS" = 15, "FC" = 17, "Virridy" = 19)) +
    labs(
      x = paste0("Measured ", target_col, " (", units, ")"),
      y = paste0("Predicted ", target_col, " (", units, ")"),
      color = "Model Group",
      #shape = "Site",
      title = paste0("Model Fold: ", fold_id)
    ) +
    theme_bw(base_size = 16) +
    xlim(min_val, max_val) +
    ylim(min_val, max_val) +
    theme(legend.position = "bottom",
          plot.title = element_text(hjust = 0.5, face = "bold"))
  
  train_val_plot
  
}

# Create a list to store all fold plots
fold_perf_plot <- list()
for (i in 1:3) {
  fold_perf_plot[[i]] <- plot_fold_performance(fold_id = i, fold_model = model[[i]], train_val_df = train_val, target_col = "TOC", units = "mg/L")
}
ggarrange(plotlist = fold_perf_plot, ncol = 2, nrow = 2, common.legend = T, legend = "bottom")

#ggsave("data/upper_clp_dss/figures/toc_modeling/performance/toc_model_fold_validation_performance.png", width = 14, height = 10)

```

# Evaluate on Test Set
DO NOT Retrain without large changes

## Calculate performance and apply to testing set

```{r}
# Convert testing and train_val datasets to matrices for prediction

test_matrix <- testing[, features]%>%
  mutate(across(everything(), as.numeric)) %>%
  as.matrix()

train_matrix <- train_val[, features]%>%
  mutate(across(everything(), as.numeric)) %>%
  as.matrix()

# Add predictions from each fold model as new columns
for (i in seq_along(model)) {
  fold_model <- model[[i]]
  col_name <- glue("{target}_guess_fold{i}")
  testing[[col_name]] <- predict(fold_model, test_matrix)
  train_val[[col_name]] <- predict(fold_model, train_matrix)
}

fold_cols <- glue("{target}_guess_fold{seq_along(model)}")
ensemble_col <- glue("{target}_guess_ensemble")
testing[[ensemble_col]] <- rowMeans(testing[, fold_cols])
train_val[[ensemble_col]] <- rowMeans(train_val[, fold_cols])

#Join
all_preds <- bind_rows(testing%>%mutate(group = "Test"), 
                       train_val%>%mutate(group = "Train"))

#compute metrics
metrics_tbl <- map_dfr(c(seq_along(model), "mean"), function(f) {
  col_name <- if (f == "mean") glue(ensemble_col) else glue("{target}_guess_fold{f}")
  
  all_preds %>%
    group_by(group) %>%
    summarise(
      model = as.character(f),
      rmse = rmse(.data[[target]], .data[[col_name]]),
      mae  = mae(.data[[target]], .data[[col_name]]),
      bias = bias(.data[[target]], .data[[col_name]]),
      .groups = "drop"
    )%>%
    ungroup()
})%>%
  arrange(group)

metrics_tbl


```


## Visualize Train Val Test

```{r}
# Function to generate a plot for a given prediction column
plot_train_val_test <- function( train_val_df, test_df, target_col, fold_id) {
  
     if (fold_id == "Ensemble") {
  # --- Collect all test predictions from every fold ---
  test_preds_all <- map_dfc(model, function(m) {
    predict(m, as.matrix(test_df[, features]))
  })
  colnames(test_preds_all) <- paste0("fold_", seq_along(model))
  
  test_preds_all <- test_preds_all %>%
    mutate(id = test_df$id,
           observed = test_df[[target_col]])
  
  test_summary <- test_preds_all %>%
    pivot_longer(starts_with("fold_"), names_to = "fold", values_to = "pred") %>%
    group_by(id, observed) %>%
    summarise(
      pred_mean = mean(pred, na.rm = TRUE),
      pred_min  = min(pred, na.rm = TRUE),
      pred_max  = max(pred, na.rm = TRUE),
      .groups = "drop"
    )
  
  # --- Training predictions (mean across folds) ---
  train_preds_all <- map_dfc(model, function(m) {
    predict(m, as.matrix(train_val_df[, features]), iteration_range = c(1, m$best_iteration))
  })
  colnames(train_preds_all) <- paste0("fold_", seq_along(model))
  
  train_summary <- train_preds_all %>%
    mutate(id = train_val_df$id,
           sensor_datetime = train_val_df$sensor_datetime,
           observed = train_val_df[[target_col]], 
           pred_mean = rowMeans(select(., starts_with("fold_")), na.rm = TRUE))

  # --- Metrics ---
  train_rmse <- rmse(train_summary$observed, train_summary$pred_mean) %>% round(2)
  train_mae  <- mae(train_summary$observed, train_summary$pred_mean) %>% round(2)
  test_rmse  <- rmse(test_summary$observed, test_summary$pred_mean) %>% round(2)
  test_mae   <- mae(test_summary$observed, test_summary$pred_mean) %>% round(2)
  
  # --- Axis limits ---
  min_val <- min(c(test_summary$observed, train_summary$observed), na.rm = TRUE)
  max_val <- max(c(test_summary$observed, train_summary$observed), na.rm = TRUE)
  plot_range <- max_val - min_val
  
  # annotation box coords
  box_width <- plot_range * 0.4
  box_height <- plot_range * 0.2
  box_xmin <- min_val + plot_range * 0.02
  box_xmax <- box_xmin + box_width
  box_ymax <- max_val - plot_range * 0.02
  box_ymin <- box_ymax - box_height
  text_x <- (box_xmin + box_xmax) / 2
  text_y1 <- box_ymax - box_height * 0.15
  text_y2 <- box_ymax - box_height * 0.5
  text_y3 <- box_ymax - box_height * 0.85
  
  # --- Plot ---
  p <- ggplot() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", size = 1.5) +
  
  # Training points (blue circles)
  geom_point(
    data = train_summary,
    aes(x = observed, y = pred_mean,
        color = "Training", shape = "Training"),
    size = 4, alpha = 0.7
  ) +
  
  # Test points (pink triangles + errorbars)
  geom_errorbar(
    data = test_summary,
    aes(x = observed, ymin = pred_min, ymax = pred_max),
    width = 0.2, color = "#E70870", alpha = 0.6
  ) +
  geom_point(
    data = test_summary,
    aes(x = observed, y = pred_mean,
        color = "Testing", shape = "Testing"),
    size = 4, alpha = 0.9
  ) +
  # Annotation box
  annotate("rect", xmin = box_xmin, xmax = box_xmax, ymin = text_y3, ymax = box_ymax ,
           fill = "white", color = "black", alpha = 1, linewidth = 0.5) +
  annotate("text", x = text_x, y = text_y1,
           label = paste0("Training samples: n = ", nrow(train_summary),
                          "\nTesting samples: n = ", nrow(test_summary)),
           color = "black", size = 4, fontface = 2) +
  annotate("text", x = text_x, y = text_y2,
           label = paste0("Testing RMSE: ", test_rmse,
                          " mg/L\nTesting MAE: ", test_mae, " mg/L"),
           color = "black", fontface = 2, size = 4.5) +
  
  labs(
    x = "Measured TOC (mg/L)",
    y = "Predicted TOC (mg/L)",
    title = "Ensemble Predictions (Mean Â± Range across folds)",
    subtitle = "CLP Samples Only",
    color = "Model Group", shape = "Model Group"
  ) +
    scale_color_manual(values = c("Training" = "#002EA3", "Testing" = "#E70870")) +
    scale_shape_manual(values = c("Training" = 16, "Testing" = 17)) + # 16=circle, 17=triangle
    xlim(min_val, max_val) +
    ylim(min_val, max_val) +
    theme_bw(base_size = 20) +
    ROSS_theme +
    theme(
      plot.title = element_text(hjust = 0.5, face = "bold", size = 18), 
      axis.title.x = element_text(size = 16),
      axis.title.y = element_text(size = 16), 
      legend.text = element_text(size = 14),
      legend.title = element_text(size = 16, face = "bold")
    )
  

  
  return(p)
}

  
  val_set_name <- paste0("val_set_", fold_id)
  val_set <- train_val_df %>% filter(id %in% get(val_set_name))
  train_set <- train_val_df %>% filter(id %nin% get(val_set_name))
  test_set <- test_df 
  
  
  #use the appropirate fold model to predict target column
  fold_model <- model[[fold_id]]
  features <- fold_model$feature_names
  # Create matrix for train/val/test datasets
  val_matrix <- val_set[, features]%>%
    mutate(across(everything(), as.numeric)) %>%
    as.matrix()
  train_matrix <- train_set[, features]%>%
    mutate(across(everything(), as.numeric)) %>%
    as.matrix()
  test_matrix <- testing[, features]%>%
    mutate(across(everything(), as.numeric)) %>%
    as.matrix()
  # Define prediction column name
  pred_col <- glue("{target}_guess_fold{fold_id}")
  
  # make predictions
  val_set[[pred_col]] <-  predict(fold_model, val_matrix)
  val_set$group <-  "Validation"
  
  train_set[[pred_col]] <-  predict(fold_model, train_matrix)
  train_set$group <-  "Train"
  
  test_set[[pred_col]] <-  predict(fold_model, test_matrix)
  test_set$group <-  "Test"
  
  data <- bind_rows(train_set, val_set, test_set)
  
  # Calculate RMSE/MAE for annotation
  train_rmse <- rmse(train_set[[target_col]],train_set[[pred_col]]) %>% round(2)
  val_rmse <- rmse(val_set[[target_col]],val_set[[pred_col]]) %>% round(2)
  train_mae <- mae(train_set[[target_col]], train_set[[pred_col]]) %>% round(2)
  val_mae <- mae(val_set[[target_col]], val_set[[pred_col]]) %>% round(2)
  test_rmse <- rmse(test_set[[target_col]], test_set[[pred_col]]) %>% round(2)
  test_mae <- mae(test_set[[target_col]], test_set[[pred_col]]) %>% round(2)
  
  
  min_val <- min(data[[target_col]], na.rm = TRUE)
  max_val <- max(data[[target_col]], na.rm = TRUE)
  plot_range <- max_val - min_val
  
  box_width <- plot_range * 0.5
  box_height <- plot_range * 0.2
  box_xmin <- min_val + plot_range * 0.02
  box_xmax <- box_xmin + box_width
  box_ymax <- max_val - plot_range * 0.02
  box_ymin <- box_ymax - box_height
  text_x <- (box_xmin + box_xmax) / 2
  text_y1 <- box_ymax - box_height * 0.15
  text_y2 <- box_ymax - box_height * 0.5
  text_y3 <- box_ymax - box_height * 0.85
  
  ggplot(data, aes(x = .data[[target_col]], y = .data[[pred_col]], color = group)) +
    geom_abline(intercept = 0, slope = 1, linetype = "dashed", size = 1.2) +
    
    # Training points (all same shape)
    geom_point(
      data = filter(data, group == "Train"),
      shape = 16,  # solid circle
      size = 4,
      alpha = 0.7) +  
    geom_point(
      data = filter(data, group == "Validation"),
      shape = 15,  # solid square
      size = 4,
      alpha = 0.7) + 
    geom_point( # Testing points (shape by id)
      data = filter(data, group == "Test"),
      shape = 17,
      #aes(shape = id),
      size = 4,
      alpha = 0.99
    ) +
    annotate("rect", xmin = box_xmin, xmax = box_xmax, ymin = box_ymin, ymax = box_ymax,
             fill = "white", color = "black", alpha = 1, linewidth = 0.5) +
    annotate("text", x = text_x, y = text_y1, label = paste0("Model Fold: ", fold_id),
             color = "black", size = 4.5, fontface = 2) +
    annotate("text", x = text_x, y = text_y2, label = paste0("Validation RMSE: ", val_rmse, 
                                                             " mg/L, MAE:", val_mae, " mg/L"),
             color = "black", size = 3, fontface = 2) +
    annotate("text", x = text_x, y = text_y3, label = paste0("Testing RMSE: ", test_rmse, 
                                                             " mg/L, MAE:", test_mae, " mg/L"),
             color = "black", fontface = 2, size = 3.5) +
    scale_color_manual(values = c("Train" = "#002EA3","Validation" = "#FFCA3A","Test" = "#E70870"),
                       breaks = c("Train", "Validation", "Test"))+
    #scale_shape_manual(values = c("ROSS" = 15, "FC" = 17, "Virridy" = 19)) +
    labs(
      x = "Measured TOC (mg/L)",
      y = "Predicted TOC (mg/L)",
       subtitle = "CLP Samples Only",
      color = "Model Group",
      #shape = "Site",
      title = 
    ) +
    xlim(min_val, max_val) +
    ylim(min_val, max_val) +
    theme(
      plot.title = element_text(hjust = 0.5, face = "bold"))+
    ROSS_theme
}

# 1. Plot each fold
for (i in seq_along(fold_cols)) {
  p <- plot_train_val_test(train_val, testing,"TOC", fold_id = i)
  plot(p)
  ggsave(glue("data/upper_clp_dss/figures/toc_modeling/performance/ross_only_model_fold_{i}.png"),
      plot = p, width = 10, height = 6, dpi = 500)
}

# 2. Single plot with all four folds
fold_plots <- lapply(seq_along(fold_cols), function(i) {
  p <- plot_train_val_test(train_val, testing,"TOC", fold_id = i)
})

ggarrange(plotlist = fold_plots, ncol = 2, nrow = 2, common.legend = TRUE, legend = "top")
ggsave("data/upper_clp_dss/figures/toc_modeling/performance/ross_only_model_all_folds_TVT.png",
       width = 14, height = 8, dpi = 500)

# 2. Plot ensemble
ensemble <- plot_train_val_test(train_val, testing,"TOC", fold_id = "Ensemble")
plot(ensemble)
ggsave("data/upper_clp_dss/figures/toc_modeling/performance/ross_only_model_ensemble.png",
       plot = ensemble, width = 12, height = 10, dpi = 500)
```

# Create global model 

```{r}
best_params <- tibble()
for(i in 1:length(model)){
  best_params <- bind_rows(best_params,  model[[i]]$model$params%>%
                             as_tibble()%>%
                             select(-c(objective, eval_metric, validate_parameters))%>%
                             mutate(fold = i))
}
print(best_params)

# Prepare DMatrix objects
dtrain <- xgb.DMatrix(data = as.matrix(train_val[, features]), label = train_val$TOC)
dtest  <- xgb.DMatrix(data = as.matrix(testing[, features]), label = testing$TOC)

watchlist <- list(train = dtrain, eval = dtest)

# Train XGBoost model with all training data and validate on testing sites
xgb_model <- xgb.train(
  params = list(
    objective = "reg:squarederror",
    max_depth = 2,
    eta = 0.01,
    gamma = 0.6,
    colsample_bytree = 0.8,
    min_child_weight = 2,
    subsample = 0.8
  ),
  data = dtrain,
  nrounds = 20000,
  watchlist = watchlist,     
  early_stopping_rounds = 1000,
  print_every_n = 100
)



```



