---
title: "Sensor Grab Munge"
author: "Sam Struthers- CSU ROSSyndicate"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(dplyr.summarise.inform = FALSE)
source("src/setup_libraries.R")

```

# Read in water chemistry data

Prepared in `01_raw_data_prep.Rmd`

```{r}
water_chem <- read_parquet("data/upper_clp_dss/modeling/ROSS_FC_water_chemistry.parquet")

wide_sensor_file <- list.files(path = "data/upper_clp_dss/sensor/prepped/", pattern = "all_sensor_data_wide_.*\\.parquet$", full.names = TRUE)%>%
  tail(1)

all_data <- read_parquet(wide_sensor_file)%>%
  mutate(year = year(DT_hourly_round))%>%
  #split by year
  split(.$year)
```

## Compute additional lagged parameters

```{r}
# custom function to calculate lagged parameters
source("src/calc_lagged_parameter.R")
#custom function to apply fdom temp correction
source("src/apply_fdom_temp_corr.R")

final_dataset <-all_data%>%
  map(., function(df){
    df %>%
      #cleaning up turbidity values (rounding to nearest value, setting min to 1 and max to 1000)
      mutate(Turbidity = round(Turbidity),
             Turbidity = if_else(Turbidity <= 0, 1, if_else(Turbidity>=1000, 1000, Turbidity)))%>%
      #applying FDOM temperature correcting/standardization
      apply_fdom_temp_corr( fdom_col = "FDOM Fluorescence",
                            temp_col = "Temperature",
                            fdom_corr_col = "FDOM_corr_25c",
                            Tr = 25,
                            rho = -0.015)%>%
      # recent means (water temp)
      calc_lagged_parameter(site_col = "site", parameter_col = "Temperature",
                            dt_col = "DT_hourly_round", lag_fun = "mean",
                            time_period = "48 hour", new_value_col = "temp_mean_48hr")%>%
      calc_lagged_parameter(site_col = "site", parameter_col = "Temperature",
                            dt_col = "DT_hourly_round", lag_fun = "mean",
                            time_period = "24 hour", new_value_col = "temp_mean_24hr")%>%
      # recent maxes (temp, fdom)
      calc_lagged_parameter(site_col = "site", parameter_col = "FDOM Fluorescence",
                            dt_col = "DT_hourly_round", lag_fun = "max",
                            time_period = "6 hour", new_value_col = "fdom_max_6hr")%>%
      calc_lagged_parameter(site_col = "site", parameter_col = "FDOM_corr_25c",
                            dt_col = "DT_hourly_round", lag_fun = "max",
                            time_period = "6 hour", new_value_col = "fdom_corr_max_6hr")%>%
      calc_lagged_parameter(site_col = "site", parameter_col = "Temperature",
                            dt_col = "DT_hourly_round", lag_fun = "max",
                            time_period = "24 hour", new_value_col = "temp_max_24hr")%>%
      # recent variance/sd (sc)
      calc_lagged_parameter(site_col = "site", parameter_col = "Specific Conductivity",
                            dt_col = "DT_hourly_round", lag_fun = "var",
                            time_period = "24 hour", new_value_col = "sc_var_24hr")%>%
      mutate(sc_sd_24hr = sqrt(sc_var_24hr))%>%
      #12 hour median turbidity (proxy for recent storm event)
      calc_lagged_parameter(site_col = "site", parameter_col = "Turbidity",
                            dt_col = "DT_hourly_round", lag_fun = "median",
                            time_period = "24 hour", new_value_col = "turb_median_12hr")
  })


```




## Select features 

```{r}

# Combine all years into one dataframe
all_sensor_data_wide <- bind_rows(final_dataset)%>%
  rename(DT_round = DT_hourly_round)

id_cols <- c("site", "DT_round", "data_avail", "year")

features <- setdiff(colnames(all_sensor_data_wide), id_cols)
#remove pH, ORP, Depth, DO
features <- features[features %nin% c("pH", "pH MV", "ORP", "Depth", "DO")]

all_sensor_data_wide <- all_sensor_data_wide%>%
  select( any_of(id_cols), any_of(features))%>% # remove unneeded parameters for TOC model 
  mutate(data_avail = if_else(rowSums(!is.na(across(all_of(features)))) == length(features), TRUE, FALSE))

```

## Save entire wide dataset

```{r}

write_parquet(all_sensor_data_wide, paste0("data/upper_clp_dss/sensor/prepped/all_sensor_data_wide_", Sys.Date(),".parquet"))
```

## plot available data vs water chem

```{r}

map(unique(water_chem$site_code), function(site_sel) {
  
  # years where site has samples
  valid_years <- water_chem %>% 
    filter(site_code == site_sel & !is.na(TOC)) %>% 
    mutate(year = year(as.Date(DT_sample))) %>% 
    distinct(year) %>% 
    pull(year)
  
  if(is_empty(valid_years)) {
    return(NULL)  # Skip this iteration if no valid years
  }
  
  p <- all_sensor_data_wide %>%
    filter(site == site_sel) %>% 
    mutate(date = as.Date(DT_round),
           year = year(date)) %>%
    filter(year %in% valid_years) %>%   # keep only years with samples
    summarize(data_avail = any(data_avail), .by = c("date")) %>%
    mutate(year = year(date)) %>%
    ggplot(aes(x = date, y = data_avail)) +
    geom_point() +
    geom_vline(
      data = water_chem %>%
        rename(site = site_code) %>%
        filter(site == site_sel & !is.na(TOC)) %>%
        mutate(date = as.Date(DT_sample),
               year = year(date)),
      aes(xintercept = date), 
      color = "red"
    ) +
    labs(title = paste("Data Availability for Site:", site_sel)) +
    facet_wrap(~year, scales = "free")
  
  print(p)
})

```


# Find match ups between grabs and sensors

This works using the wide dataset and finds the most recent (forward looking) timestep where all sensor parameters (features) are available. In the instance where there is no data after a sample is collected (ie a sensor is removed for the year), it will find the most recent (backward looking) timestep. If no data is available, a row will be added with NA values for the sensor data but will contain the water chemistry data.

```{r}
#trim waterchem to only samples with TOC and needed columns
water_chem <- water_chem%>%
  filter(!is.na(TOC))%>%
  select(site_code, DT_sample, TOC, ChlA, collector)

#loop over waterchem to form matchup dataset
modeling_data <- map(1:nrow(water_chem), function(i) {
  # Get the current water chemistry row
  current_row <- water_chem[i, ]
  
  if(i/25 == round(i/25)) {
    cat("Processing row", i, "of", nrow(water_chem), "\n")
  }
  
  #get data from the day of sampling
  site_data <- all_sensor_data_wide %>%
    filter(site == current_row$site_code & 
             date(DT_round) == date(current_row$DT_sample) &
             data_avail == TRUE)
  
  # Find the most recent (forward looking) sensor data
  most_recent_sensor <- site_data %>%
    filter(DT_round >= current_row$DT_sample)%>%
    arrange(DT_round) %>%
    slice_head(n = 1)
  if(nrow(most_recent_sensor) > 0){
    #if this is greater than 3 hours, or no data found, look backwards
    fwd_time_diff <- as.numeric(difftime(most_recent_sensor$DT_round, current_row$DT_sample, units = "hours"))
    if (fwd_time_diff > 3) {
      # Find the most recent sensor data (closest before sampling)
      backward_check <- site_data %>%
        filter(DT_round <= current_row$DT_sample)%>%
        arrange(desc(DT_round)) %>%
        slice_head(n = 1)
      # Calculate backward time difference
      bwd_time_diff <- as.numeric(difftime(current_row$DT_sample, backward_check$DT_round, units = "hours"))
      # Check if backward match exists
      if(nrow(backward_check) > 0) {
        # If the backward time difference is less than 1 hour or forward is > 5 hours, use backward match
        if(bwd_time_diff < 1 | fwd_time_diff > 5) {
          most_recent_sensor <- backward_check
        }else{
          # If not, keep the forward match (even if > 3 hours)
          # most_recent_sensor remains unchanged
        }
      }
    }
  }
  
  # If no forward match found, look backwards
  if (nrow(most_recent_sensor) == 0) {
    # Find the most recent sensor data (closest before sampling)
    most_recent_sensor <- site_data %>%
      filter(DT_round <= current_row$DT_sample)%>%
      arrange(desc(DT_round)) %>%
      slice_head(n = 1)
  }
  
  # If still no matching data is found, add NA values
  if (nrow(most_recent_sensor) == 0) {
    most_recent_sensor <- tibble(
      site = current_row$site_code,
      DT_round = NA,
      data_avail = NA,
      !!!setNames(rep(list(NA_real_), length(features)), features)
    )
  }
  
  # Combine the current water chemistry row with the sensor data
  bind_cols(current_row, most_recent_sensor)
}) %>% 
  bind_rows()%>%
  #fix column names for later use
  dplyr::select(id = site,
                sensor_datetime = DT_round, 
                lab_datetime = DT_sample, 
                any_of(features),everything())


```

## Check Matches

- We are losing quite a few samples for periods that have not been verified yet. In particular, we are missing a lot of samples from CHD, SFM and Udall in 2024. It is likely worth looking into which flags we can generally ignore to give ourselves more data before everything is completely manually verified. 


```{r}
# Count of how many did not match (sensor_datetime is NA)
TOC_samples <- modeling_data %>%
  filter(!is.na(TOC)) 

missing_samples <- TOC_samples%>%
  mutate(year = year(lab_datetime))%>%
  group_by(year, id) %>%
  summarise(n_missing = sum(is.na(sensor_datetime)),
            n_missing_fc = sum(is.na(sensor_datetime)&collector == "FC"),
            n_missing_ross = n_missing-n_missing_fc,
            .groups = "drop")%>%
  arrange(desc(n_missing))%>%
  filter(n_missing > 0)

FC_missing_samples <- TOC_samples%>%
  filter(collector == "FC" & is.na(sensor_datetime))

cat("Number of samples with no match up:\n", sum(missing_samples$n_missing), " of ", nrow(TOC_samples), "\n", nrow(FC_missing_samples), " are FC samples with no match (ie sensor may not be deployed)" )
print(missing_samples, n = Inf)

#create a histogram of the time difference between sensor_datetime and lab_datetime
time_diff <- TOC_samples %>%
  filter(!is.na(sensor_datetime)) %>%
  mutate(time_difference = as.numeric(difftime(sensor_datetime, lab_datetime, units = "hours")),
         abs_time_difference = abs(time_difference))

ggplot(time_diff, aes(x = time_difference)) +
  geom_histogram(binwidth = 1, fill = "blue", color = "black") +
  labs(title = "Histogram of Time Difference Between Sensor and Lab Datetime",
       x = "Time Difference (hours)",
       y = "Count") +
  ROSS_theme

#which samples are greater than 3 hours difference
large_time_diff <- time_diff %>%
  filter(abs_time_difference > 3) %>%
  arrange(desc(abs_time_difference))%>%
  select(id, lab_datetime, sensor_datetime, time_difference, abs_time_difference, collector)
large_time_diff

```


## Create example plot of sensor TS 
Add grab sample DT as dashed black line and sensor match up as solid red line

```{r}

example_site <- "sfm"
site_title <- "S FORK"

start_dt <- ymd_hm("2024-04-01 00:00", tz = "MST")
end_dt <- ymd_hm("2024-12-05 00:00", tz = "MST")

example_data <- all_sensor_data_wide%>%
  filter(site == example_site)%>%
  filter(between(DT_round,start_dt, end_dt))%>%
  mutate(date = as.Date(DT_round))%>%
  pivot_longer(cols = c("Chl-a Fluorescence", "Temperature", "FDOM Fluorescence", "Turbidity", "Specific Conductivity"), names_to = "parameter", values_to = "value")

example_grab <- modeling_data%>%
  filter(id == example_site)%>%
  filter(between(lab_datetime, start_dt, end_dt))%>%
  mutate(lab_date = as.Date(lab_datetime),
         sensor_date = as.Date(sensor_datetime))

example_ts_grab <- ggplot(example_data, aes(x = DT_round, y = value)) +
  geom_line(color = "#E70870") +
  geom_vline(data = example_grab, aes(xintercept = lab_datetime, color = is.na(sensor_datetime)), linetype = "dashed") +
  #geom_vline(data = example_grab %>% filter(!is.na(sensor_datetime)), aes(xintercept = sensor_datetime), linetype = "solid", color = "#256BF5") +
  facet_wrap(~parameter, scales = "free_y", ncol = 1) +
  labs(title = paste("Example Sensor Time Series at Site:", site_title),
       x = "Date",
       y = "Sensor Value") +
  ROSS_theme

example_ts_grab

ggsave(plot = example_ts_grab, "data/upper_clp_dss/figures/example_sensor_timeseries_pfal.png",
       width = 12, height = 8)

```



# Save Final Dataset

## Saving ROSS only set

```{r}

ROSS_model_data <- modeling_data%>%
  #removing samples with no match up
  filter(!is.na(sensor_datetime))%>%
  mutate(sensor_datetime = as.character(sensor_datetime), 
         lab_datetime = as.character(lab_datetime))

write_csv(ROSS_model_data, paste0("data/upper_clp_dss/modeling/parsed_data_ROSS_", Sys.Date(),".csv"))

```



## Binding with Virridy 

```{r}
virridy_data <- read_csv("data/upper_clp_dss/modeling/parsed_data_virridy.csv")%>%
  dplyr::select(-"...1" )%>%
  mutate(id = as.character(mw_id))%>%
  select(-mw_id)%>%
  mutate(collector = "Virridy")

all_matched_data <- ROSS_model_data%>%
  bind_rows(virridy_data)

write_csv(all_matched_data, "data/upper_clp_dss/modeling/parsed_data_ROSS_virridy.csv")

```


# Explore new features

## Comparing FDOM Corrected values to original RFUs

```{r}
modeling_data_new <- modeling_data %>%
  mutate(
    doy = yday(sensor_datetime),
    Turbidity = round(Turbidity),
    Turbidity = if_else(Turbidity == 0, 1, Turbidity),
    watershed = if_else(id %in% c("pfal", "pbr", "pman", "pbd", "chd", "joei", "cbri", "penn", "lbea", "sfm"),
                        "UpperPoudre", "LowerPoudre"),
    # FDOM_corr_25c features
    f_temp    = FDOM_corr_25c / Temperature,
    f_sc      = FDOM_corr_25c / `Specific Conductivity`,
    f_turb    = FDOM_corr_25c / Turbidity, 
    turb_f    = Turbidity / FDOM_corr_25c, 
    temp_f    = Temperature / FDOM_corr_25c, 
    f_log     = log1p(FDOM_corr_25c), 
    temp_x_f  = Temperature * FDOM_corr_25c,
    temp_x_turb = Temperature * Turbidity,
    f_x_sc    = FDOM_corr_25c * `Specific Conductivity`,
    f_x_turb  = FDOM_corr_25c * Turbidity,
    f2_temp   = (FDOM_corr_25c^2) * Temperature,
    
    # Duplicates using FDOM Fluorescence
    f_temp_org    = `FDOM Fluorescence` / Temperature,
    f_sc_org      = `FDOM Fluorescence` / `Specific Conductivity`,
    f_turb_org    = `FDOM Fluorescence` / Turbidity, 
    turb_f_org   = Turbidity / `FDOM Fluorescence`, 
    temp_f_org    = Temperature / `FDOM Fluorescence`, 
    f_log_org     = log1p(`FDOM Fluorescence`), 
    temp_x_f_org  = Temperature * `FDOM Fluorescence`,
    f_x_sc_org    = `FDOM Fluorescence` * `Specific Conductivity`,
    f_x_turb_org  = `FDOM Fluorescence` * Turbidity,
    f2_temp_org   = (`FDOM Fluorescence`^2) * Temperature, 
    #doy sin
    sin_doy = sin(2 * pi * doy / 365.25), 
    cos_doy = cos(2 * pi * doy / 365.25)
  )


# Define feature "bases" (no suffix)
feature_bases <- c("f_temp","f_sc","f_turb","turb_f","temp_f","f_log",
                   "temp_x_f","f_x_sc","f_x_turb","f2_temp")

# Map each base to its 3 variants
feature_sets <- tibble(
  base = rep(feature_bases, each = 2),
  version = rep(c("corrected","original"), times = length(feature_bases))
)%>%
  mutate(feature_name = ifelse(version == "original", paste0(base, "_org"), base))

# Run models for each feature
r2_table <- map_dfr(feature_sets$feature_name, function(feat){
  modeling_data_new %>%
    group_by(watershed) %>%
    do({
      fit <- lm(TOC ~ .data[[feat]], data = .)
      tibble(
        feature_name = feat,
        adj_r2 = summary(fit)$adj.r.squared
      )
    })
})

# Join with feature mapping
r2_table <- r2_table %>%
  left_join(feature_sets, by = c("feature_name")) %>%
  select(base, version, watershed, adj_r2)

# Reshape: wide by version × watershed
r2_wide <- r2_table %>%
  pivot_wider(
    names_from = c(version, watershed),
    values_from = adj_r2
  )%>%
  select(base, original_UpperPoudre, corrected_UpperPoudre, original_LowerPoudre,
         corrected_LowerPoudre)%>%
  arrange(desc(corrected_UpperPoudre))%>%
  mutate(diff_upper = corrected_UpperPoudre - original_UpperPoudre,
         diff_lower = corrected_LowerPoudre - original_LowerPoudre)

r2_wide

r2_wide %>%
  tidyr::pivot_longer(
    cols = c(original_UpperPoudre, corrected_UpperPoudre,
             original_LowerPoudre, corrected_LowerPoudre),
    names_to = c("version", "watershed"),
    names_pattern = "(original|corrected)_(.*)",
    values_to = "adj_r2"
  ) %>%
  tidyr::pivot_wider(names_from = version, values_from = adj_r2) %>%
  ggplot(aes(x = original, y = corrected, color = watershed, label = base)) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  geom_point(size = 3) +
  geom_text(vjust = -0.7, size = 3) +
  labs(x = "Original R²", y = "Corrected FDOM R²",
       title = "Comparison of Adjusted R² by FDOM method") +
  theme_minimal()
```

## Checking relationships for new features with TOC

```{r}

id_cols <- c("id", "sensor_datetime", "lab_datetime", "collector",
             "data_avail", "doy", "site_code","TOC", "ChlA", "watershed", "year")

#new features 
new_features <- setdiff(names(modeling_data_new), c(id_cols))
#remove anything with org in it
new_features <- new_features[!str_detect(new_features, "_org")]
#dropping additional ones
new_features <- new_features[!new_features %in% c("sc_sd_24hr", "temp_mean_48hr", #lagged parameters
                                                  "temp_x_turb")]


plot_list <- list()
for(i in new_features){
  
  plot_list[[i]] <- ggplot(modeling_data_new, 
                           aes(x = .data[[i]], y = TOC, color = id)) +
    geom_point_interactive(
      aes(
        tooltip = paste("Site:", id, 
                        "<br>", i, ":", round(.data[[i]], 2), 
                        "<br>TOC:", round(TOC, 2)),
        data_id = paste(id, TOC, sep = "_")   # <- shared ID across plots
      )
    ) +
    geom_smooth(method = "lm", se = FALSE, color = "blue") +
    stat_regline_equation(
      label.x.npc = "left", label.y.npc = 0.9,
      aes(label = paste( ..adj.rr.label.., sep = "~~~~")),
      color = "blue"
    ) +
    labs(x = i, y = "TOC (mg/L)") +
    theme(legend.position = "none") +
    guides(color = "none") +
    facet_wrap(~watershed, scales = "free") +
    ROSS_theme
  
  if(i %in% c("turb_median_12hr", "turb_mean_12hr")){
    plot_list[[i]] <- plot_list[[i]] + xlim(0,100)
  }
  if(i %in% c("f_temp")){
    plot_list[[i]] <- plot_list[[i]] + xlim(0,2.5)
  }
}

# Combine with patchwork
g <- wrap_plots(plot_list, guides = "collect")+
  plot_annotation(title = "FDOM Corr Derived Parameters",
                  subtitle = "Colored by Site",
                  caption = "Dashed line is linear regression fit with adjusted R2") 

g

plot_list$sin_doy

doy_data <- tibble(doy = 1:365)
# make a plot of doy and sin(doy) and cos(doy)
ggplot( doy_data, aes(x = doy)) +
  geom_point(aes(y = sin(2 * pi * doy / 365.25)), color = "blue") +
  geom_point(aes(y = cos(2 * pi * doy / 365.25)), color = "red") +
  labs(title = "Sine (blue) and Cosine (red) of Day of Year",
       x = "Day of Year",
       y = "Value") +
  ROSS_theme


```

