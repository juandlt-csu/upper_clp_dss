---
title: "01_data_prep"
author: "Sam Struthers- CSU ROSSyndicate"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(dplyr.summarise.inform = FALSE)
source("src/setup_libraries.R")
source("src/apply_cleaning_filters.R")
source("src/apply_interpolation_missing_data.R")
source("src/apply_low_pass_binomial_filter.R")
source("src/apply_timestep_median.R")
```


# Pull in & Clean up sensor data

## Set Data Targets (sites and parameters)

```{r}
site_levels <- c("salyer", "udall", "riverbend", "cottonwood","riverbend_virrridy", "cottonwood_virridy", "elc",
                 "archery", "archery_virridy" ,  "riverbluffs", "joei", "cbri", "chd", "pfal", "sfm", "pbr", "pman", "pbd", "penn", "lbea", "springcreek", "boxcreek")

parameter_levels <- c("Chl-a Fluorescence", "Depth", "DO", "ORP", "pH","Specific Conductivity", "Temperature", "Turbidity", "FDOM Fluorescence") 

all_combinations <- crossing(
  site = site_levels,
  parameter = parameter_levels
)
site_level_string <- paste(site_levels, collapse = "|")
parameter_level_string <- paste(parameter_levels, collapse = "|")
```


merge 2023 + 2024 data for sites OI

## 2023 dataset

```{r}
#load in 2023 verified data
# Pulling in the manually verified data
verified_names <- list.files(
  here("data", "manual_data_verification", "2023_cycle", 
       "in_progress", "virridy_verification", "verified_directory"),
  full.names = T)

finalized_dataset_2023 <- verified_names %>% 
  map(\(file_path){
    site_parameter_df <- read_rds(file_path) %>%
      fix_site_names()
    return(site_parameter_df)
  }) %>%
  set_names(map_chr(., ~ paste0(unique(.x$site), "-", unique(.x$parameter)))) %>%
  keep_at(imap_lgl(., ~ grepl(site_level_string, .y))) %>%
  keep_at(imap_lgl(., ~ grepl(parameter_level_string, .y))) 
# Pulling in the post verified data
post_verified_names <- list.files(
  here("data", "manual_data_verification", "2023_cycle",
       "in_progress", "virridy_verification", "post_verified_directory"),
  full.names = T)
post_finalized_dataset_2023 <- post_verified_names %>% 
  map(\(file_path){
    site_parameter_df <- read_rds(file_path) %>%
      fix_site_names()
    return(site_parameter_df)
  }) %>%
  set_names(map_chr(., ~ paste0(unique(.x$site), "-", unique(.x$parameter)))) %>%
  keep_at(imap_lgl(., ~ grepl(site_level_string, .y))) %>%
  keep_at(imap_lgl(., ~ grepl(parameter_level_string, .y))) 
# Combining all 2023 data by preferentially using post verified data
post_names <- intersect(names(post_finalized_dataset_2023), names(finalized_dataset_2023))
non_post_names <- setdiff(names(finalized_dataset_2023), post_names)
dataset_2023 <- c(
  post_finalized_dataset_2023[post_names], 
  finalized_dataset_2023[non_post_names]
)
all_data_2023 <- dataset_2023 %>%
  bind_rows() %>% 
  data.table() %>%
  fix_site_names() %>% 
  mutate(
    clean_mean = case_when(
      is.na(flag) & verification_status == "PASS" ~ mean,
      is.na(flag) & verification_status == "FAIL" ~ NA,
      !is.na(flag) & verification_status == "PASS" ~ NA,
      !is.na(flag) & verification_status == "FAIL" ~ mean
    ),
    clean_flag = case_when(
      is.na(flag) & verification_status == "PASS" ~ NA,
      is.na(flag) & verification_status == "FAIL" ~ "MANUAL FLAG",
      !is.na(flag) & verification_status == "PASS" ~ flag,
      !is.na(flag) & verification_status == "FAIL" ~ NA
    )
  ) %>%
  filter( !is.na(site),
    # Filter based on DT. The 2023 data is in MST.
    DT_round >= as.POSIXct("2023-01-01 00:00:00", tz = "MST") & DT_round <= as.POSIXct("2023-12-31 11:59:59", tz = "MST")
  ) %>%
  #fixing virridy site names
  mutate(site = gsub(" virridy","_virridy", site  ))%>%
  dplyr::select(DT_round, DT_join, site, parameter, mean = clean_mean, flag = clean_flag,last_site_visit)
```

### Cleaning tasks (interpolation, smoothing, hourly median)

```{r}
all_data_2023_final <- apply_interpolation_missing_data(df = all_data_2023, new_value_col = "mean_filled", max_gap = 4, method = "spline")%>%
  apply_low_pass_binomial_filter(value_col = "mean_filled", new_value_col = "smoothed_mean")%>%
  apply_timestep_median(df = ., timestep  = "1 hour", value_col = "smoothed_mean", new_value_col = "hourly_median")

```

### Save 2023 Data & Clean Up

```{r}
write_parquet(all_data_2023_final, paste0("data/upper_clp_dss/sensor/prepped/all_2023_sensor_data_", Sys.Date(),".parquet"))

rm(all_data_2023, dataset_2023, finalized_dataset_2023, post_finalized_dataset_2023, post_verified_names)
gc()
```


## 2024 dataset

This dataset's verification is still in progress and may need to be rerun as additional data is confirmed

```{r}
#load in 2024 verified data
# Pulling in the manually verified data
verified_names <- list.files(
  here("data", "manual_data_verification", "2024_cycle", 
       "in_progress", "verified_directory"),
  full.names = T)

verified_dataset_2024 <- verified_names %>% 
  map(\(file_path){
    site_parameter_df <- read_rds(file_path)
    return(site_parameter_df)
  }) %>%
  set_names(map_chr(., ~ paste0(unique(.x$site), "-", unique(.x$parameter)))) %>%
  keep_at(imap_lgl(., ~ grepl(site_level_string, .y))) %>%
  keep_at(imap_lgl(., ~ grepl(parameter_level_string, .y))) 
# Pulling in the pre verified data
pre_verified_names <- list.files(
  here("data", "manual_data_verification", "2024_cycle",
       "in_progress", "all_data_directory"),
  full.names = T)
pre_verified_dataset_2024 <- pre_verified_names %>% 
  map(\(file_path){
    site_parameter_df <- read_rds(file_path)
    return(site_parameter_df)
  }) %>%
  set_names(map_chr(., ~ paste0(unique(.x$site), "-", unique(.x$parameter)))) %>%
  keep_at(imap_lgl(., ~ grepl(site_level_string, .y))) %>%
  keep_at(imap_lgl(., ~ grepl(parameter_level_string, .y))) 


# Combining all 2024 data by preferentially using post verified data
ver_names <- intersect(names(verified_dataset_2024), names(pre_verified_dataset_2024))
non_ver_names <- setdiff(names(pre_verified_dataset_2024), ver_names)
dataset_2024 <- c(
  verified_dataset_2024[ver_names], 
  pre_verified_dataset_2024[non_ver_names]
)
all_data_2024 <- dataset_2024 %>%
  bind_rows() %>% 
  data.table() %>%
  mutate(
    clean_mean = case_when(
      #verified data
      verification_status == "OMIT"  ~ NA,
      verification_status == "PASS" ~ mean,
      verification_status == "FLAGGED" ~ mean, 
      #unverified data
      is.na(verification_status) & is.na(flag) ~ mean, # no auto flag and not verified
      is.na(verification_status) & !is.na(flag) ~ NA #  auto flag and not verified
    ),
    clean_flag = case_when(
      #verified data
      verification_status == "OMIT"  ~ NA,
      verification_status == "PASS" ~  NA,
      verification_status == "FLAGGED" ~ user_flag,
      #unverified data
      is.na(verification_status) & is.na(flag) ~ NA, # no auto flag and not verified
      is.na(verification_status) & !is.na(flag) ~ flag #  auto flag and not verified
    )
  ) %>%
  filter( !is.na(site),
    # Filter based on DT. The 2024 data is in MST.
    DT_round >= as.POSIXct("2024-01-01 00:00:00", tz = "MST") & DT_round <= as.POSIXct("2024-12-31 11:59:59", tz = "MST")
  ) %>%
  mutate(last_site_visit = with_tz(last_site_visit, tzone = "MST"))%>%
  dplyr::select(DT_round, DT_join, site, parameter, mean = clean_mean, flag = clean_flag, last_site_visit)
```


### Cleaning tasks (interpolation, smoothing, hourly median)

```{r}
all_data_2024_final <- all_data_2024%>%
  apply_interpolation_missing_data(df = ., new_value_col = "mean_filled", max_gap = 4, method = "spline")%>%
  apply_low_pass_binomial_filter(value_col = "mean_filled", new_value_col = "smoothed_mean")%>%
  apply_timestep_median(df = ., timestep  = "1 hour", value_col = "smoothed_mean", new_value_col = "hourly_median")

```

### Save 2024 Data & Clean Up

```{r}
write_parquet(all_data_2024_final, paste0("data/upper_clp_dss/sensor/prepped/all_2024_sensor_data_", Sys.Date(),".parquet"))

rm(all_data_2024, dataset_2024, pre_verified_dataset_2024, pre_verified_names, verified_dataset_2024, verified_names)
gc()
```

## 2025 dataset (auto qaqc only)

Skip to `Loading in pre-processed 2025 data` if the auto qaqc pipeline has already been run for 2025

### Non HydroVu log parsing

These sites do not livestream to HydroVu so all the data is stored on Vulink or AquaTroll logs. The Vulink logs cannot be uploaded to HydroVu so we will pull them in manually here

```{r}

source("src/parse_insitu_html_log.R")
#sites that do not livestream
sites <- c("joei", "cbri","chd", "pfal", "sfm", "pbr", "pman") 

troll_files <- list.files(path = "data/sensor_data/2025/", pattern = "troll", full.names = T)%>%
  #only look for our upper sites that do not livestream in any capacity
  str_subset(paste(sites, collapse = "|"))

troll_data <- map(troll_files, parse_insitu_html_log) %>%
  bind_rows()

vulink_files <- list.files(path = "data/sensor_data/2025/", pattern = "vulink", full.names = T)%>%
  #only look for our upper sites that do not livestream in any capacity
  str_subset(paste(sites, collapse = "|"))

vulink_data <- map(vulink_files, parse_insitu_html_log) %>%
  bind_rows()

#always start with troll data and then fill in where needed with vulink data
log_data_2025 <- troll_data %>%
  #find the data that is not in the troll data but is in the vulink log and bind in
  bind_rows(anti_join(vulink_data, ., by = c("site", "parameter", "DT_join")))%>%
  #check for duplicates
   distinct(.keep_all = TRUE) %>%
# #format to match other data
  split(f = list(.$site, .$parameter), sep = "-") %>%
   keep(~nrow(.) > 0)


```

### HydroVu Pull

#### Setup

```{r}
# Configure your directory paths
staging_directory <- "data/manual_data_verification/2025_cycle/hydro_vu_pull/raw_data"  # Where raw data will be stored
flagged_directory <- "data/manual_data_verification/2025_cycle/hydro_vu_pullflagged_data" # Where flagged data will be saved
temp_directory <- "data/manual_data_verification/2025_cycle/hydro_vu_pull/temp_files" # Temporary processing files
final_directory <- "data/manual_data_verification/2025_cycle/hydro_vu_pull/final_output"       # Final processed data

# Configure your threshold files
sensor_thresholds_file <- "data/field_notes/qaqc/sensor_spec_thresholds.yml"
seasonal_thresholds_file <- "data/field_notes/qaqc/updated_seasonal_thresholds_2025_sjs.csv" #updated sensor thresholds with some manual changes for sites with minimal data

# Configure your credentials files
mwater_creds_file <- "creds/mWaterCreds.yml"
hydrovu_creds_file <- "creds/HydroVuCreds.yml"

# Configure date range for data retrieval
start_date <- "2025-01-01 00:00:00"  # MST
end_date <- "2025-07-05 23:59:59"    # MST

time_zone = "America/Denver"

# Configure sites to process
sites_to_process <- c("archery",  "boxcreek", "cottonwood", "elc",  
                       "pbd", "riverbluffs", "riverbend", "salyer", 
                      "springcreek", "udall")

# Configure parallel processing
max_workers <- 4  
# Maximum number of parallel workers# set up parallel processing
num_workers <- min(availableCores() - 1, max_workers)
plan(multisession, workers = num_workers)
furrr_options(
  globals = TRUE,
  packages = c("arrow", "data.table", "httr2", "tidyverse", "lubridate", "zoo",
               "padr", "stats", "RcppRoll", "yaml", "here", "fcw.qaqc")
)

# suppress scientific notation for consistent formatting
options(scipen = 999)
# read threshold data
sensor_thresholds <- read_yaml(sensor_thresholds_file)
season_thresholds <- read_csv(seasonal_thresholds_file, show_col_types = FALSE)

# read API credentials
mWater_creds <- read_yaml(mwater_creds_file)
hv_creds <- read_yaml(hydrovu_creds_file)

# authenticate access to HydroVu API
hv_token <- hv_auth(client_id = as.character(hv_creds["client"]),
                    client_secret = as.character(hv_creds["secret"]))


# pull field data from mWater API
mWater_data <- load_mWater(creds = mWater_creds)

# grab field notes with proper timezone handling
all_field_notes <- grab_mWater_sensor_notes(mWater_api_data = mWater_data) %>%
  mutate(DT_round = with_tz(DT_round, tzone = "UTC"),
         last_site_visit = with_tz(last_site_visit, tzone = "UTC"),
         DT_join = as.character(DT_round))

# grab sensor malfunction records
sensor_malfunction_notes <- grab_mWater_malfunction_notes(mWater_api_data = mWater_data) %>%
  mutate(start_DT = with_tz(start_DT, tzone = "UTC"),
         end_DT = with_tz(end_DT, tzone = "UTC"))
```

#### Pulling 2025 HV data

```{r}
###### ALREADY RUN for 07/2025 #######
# get HydroVu site information
# hv_sites <- hv_locations_all(hv_token) %>%
#   filter(!grepl("vulink", name, ignore.case = TRUE))

###### ALREADY RUN for 07/2025 #######

# convert dates to proper timezone
mst_start <- ymd_hms(start_date, tz = time_zone)
mst_end <- ymd_hms(end_date, tz = time_zone)


###### ALREADY RUN for 07/2025 #######
# 
# # pull data for each site
# walk(sites_to_process,
#      function(site) {
#        message("Requesting HV data for: ", site)
#        api_puller(
#          site = site,
#          start_dt = with_tz(mst_start, tzone = "UTC"),
#          end_dt = with_tz(mst_end, tzone = "UTC"),
#          api_token = hv_token,
#          dump_dir = staging_directory, 
#         hv_sites_arg = hv_sites, 
#         network = "all"
#        )
#      }
# )
##### Already RUN for 07/2025 ######

# load all raw data files
hv_data <- list.files(staging_directory, full.names = TRUE) %>%
  future_map_dfr(function(file_path){
    site_df <- read_parquet(file_path, as_data_frame = TRUE)
    return(site_df)
  }, .progress = TRUE)

# preprocess and standardize data
hv_data_2025 <- hv_data %>%
  data.table() %>%
  dplyr::select(-id) %>%
  mutate(units = as.character(units)) %>%
  filter(!grepl("vulink", name, ignore.case = TRUE),
         #only keep parameters of interest
         grepl(parameter_level_string, parameter, ignore.case = TRUE),
         #remove pH MV and Level
         !grepl("Level|MV", parameter, ignore.case = T)) %>%
  mutate(
    DT = timestamp,
    DT_round = round_date(DT, "15 minutes"),
    DT_join = as.character(DT_round),
    site = tolower(site)
  ) %>%
  dplyr::select(-name) %>%
  distinct(.keep_all = TRUE) %>%
  # split into site-parameter combinations for parallel processing
  split(f = list(.$site, .$parameter), sep = "-") %>%
  keep(~nrow(.) > 0)
```

### Auto QAQC pipeline for 2025 data

```{r}
# tidy raw data (default 15-minute intervals)

data_2025 <- c(hv_data_2025, log_data_2025)

tidy_data <- data_2025 %>%
  future_map(~tidy_api_data(api_data = .), .progress = TRUE) %>%
  keep(~!is.null(.))

# add field notes to tidied data
combined_data <- tidy_data %>%
  future_map(~add_field_notes(df = ., notes = all_field_notes), .progress = TRUE)

# generate summary statistics
summarized_data <- combined_data %>%
  map(~generate_summary_statistics(.))

# process data in chunks for memory efficiency
summarized_data_chunks <- split(1:length(summarized_data),
                                ceiling(seq_along(1:length(summarized_data))/10))

single_sensor_flags <- list()

for (chunk_idx in seq_along(summarized_data_chunks)) {
  message("\n=== Processing chunk ", chunk_idx, " of ", length(summarized_data_chunks), " ===")
  
  indices <- summarized_data_chunks[[chunk_idx]]
  chunk_data <- summarized_data[indices]
  
  # apply single-parameter flags
  chunk_results <- chunk_data %>%
    map(
      function(data) {
        flagged_data <- data %>%
          data.table(.) %>%
          # flag field visits
          add_field_flag(df = .) %>%
          # flag missing/NA values
          add_na_flag(df = .) %>%
          # flag dissolved oxygen noise patterns
          find_do_noise(df = .) %>%
          # flag repeating/stuck values
          add_repeat_flag(df = .) %>%
          # flag depth shifts (sonde movement)
          add_depth_shift_flag(df = ., level_shift_table = all_field_notes, post2024 = TRUE) %>%
          # flag sensor drift (FDOM, Chl-a, Turbidity)
          add_drift_flag(df = .)
        
        # apply sensor specification flags if thresholds exist
        if (unique(data$parameter) %in% names(sensor_thresholds)) {
          flagged_data <- flagged_data %>%
            data.table(.) %>%
            add_spec_flag(df = ., spec_table = sensor_thresholds)
        }
        
        # apply seasonal threshold flags if available
        if (unique(data$parameter) %in% unique(season_thresholds$parameter)) {
          flagged_data <- flagged_data %>%
            data.table(.) %>%
            add_seasonal_flag(df = ., threshold_table = season_thresholds)
        }
        
        return(flagged_data)
      },
      .progress = TRUE
    )
  
  single_sensor_flags <- c(single_sensor_flags, chunk_results)
  
  if (chunk_idx < length(summarized_data_chunks)) {
    gc()  # garbage collection between chunks
    Sys.sleep(0.1)
  }
}

# combine single-parameter flags by site
intrasensor_flags <- single_sensor_flags %>%
  rbindlist(fill = TRUE) %>%
  split(by = "site")

# process inter-parameter flags in chunks
intrasensor_data_chunks <- split(1:length(intrasensor_flags),
                                 ceiling(seq_along(1:length(intrasensor_flags))/3))

intrasensor_flags_list <- list()
for (chunk_idx in seq_along(intrasensor_data_chunks)) {
  message("\n=== Processing chunk ", chunk_idx, " of ", length(intrasensor_data_chunks), " ===")
  
  indices <- intrasensor_data_chunks[[chunk_idx]]
  chunk_data <- intrasensor_flags[indices]
  
  chunk_results <- chunk_data %>%
    map(
      function(data) {
        flagged_data <- data %>%
          data.table() %>%
          # flag when water temperature below freezing
          add_frozen_flag(.) %>%
          # check for overlapping flags and resolve
          intersensor_check(.) %>%
          # flag potential sensor burial
          add_burial_flag(.) %>%
          # flag when sonde is above water surface
          add_unsubmerged_flag(.)
        
        return(flagged_data)
      }
    ) %>%
    rbindlist(fill = TRUE) %>%
    mutate(flag = ifelse(flag == "", NA, flag)) %>%
    split(f = list(.$site, .$parameter), sep = "-") %>%
    purrr::discard(~ nrow(.) == 0) %>%
    # add known sensor malfunction periods
    map(~add_malfunction_flag(df = ., malfunction_records = sensor_malfunction_notes))
  
  intrasensor_flags_list <- c(intrasensor_flags_list, chunk_results)
  
  if (chunk_idx < length(intrasensor_data_chunks)) {
    gc()
    Sys.sleep(0.1)
  }
}

# save intermediate results
iwalk(intrasensor_flags_list, 
      ~write_csv(.x, file.path(temp_directory, paste0(.y, ".csv"))))


site_order_list <- load_site_order(here("data", "field_notes", "qaqc", "site_order_2025.yml"))

# apply network-level quality control
network_flags <- intrasensor_flags_list %>%
  # network check compares patterns across sites
  purrr::map(~network_check(df = .,network = "all", intrasensor_flags_arg = intrasensor_flags_list, site_order_arg = site_order_list)) %>%
  rbindlist(fill = TRUE) %>%
  # clean up flag column formatting
  tidy_flag_column() %>%
  split(f = list(.$site, .$parameter), sep = "-") %>%
  # add suspect data flags for isolated anomalies
  purrr::map(~add_suspect_flag(.)) %>%
  rbindlist(fill = TRUE)


# final data cleaning and preparation
v_final_flags <- network_flags %>%
  # Remove isolated suspect flags (single point anomalies)
  dplyr::mutate(auto_flag = ifelse(
    is.na(auto_flag), NA,
    ifelse(auto_flag == "suspect data" & 
           is.na(lag(auto_flag, 1)) & 
           is.na(lead(auto_flag, 1)), NA, auto_flag)
  )) %>%
  # select final columns
  dplyr::select(c("DT_round", "DT_join", "site", "parameter", "mean", "units", 
                  "n_obs", "spread", "auto_flag", "mal_flag", "sonde_moved",
                  "sonde_employed", "season", "last_site_visit")) %>%
  # clean up empty flags
  dplyr::mutate(auto_flag = ifelse(is.na(auto_flag), NA, 
                                   ifelse(auto_flag == "", NA, auto_flag))) %>%
  # split back into site-parameter combinations
  split(f = list(.$site, .$parameter), sep = "-") %>%
  keep(~nrow(.) > 0)

# save final processed data
iwalk(v_final_flags, 
      ~write_csv(.x, file.path(final_directory, paste0(.y, ".csv"))))


```

### Loading in pre-processed 2025 data

If auto qaqc has already been run for 2025, skip to this chunk

```{r}
#load in 2025 unverified but Auto QAQC'ed data
# Pulling in the manually verified data

flagged_2025_files <- list.files(
  here("data", "manual_data_verification", "2025_cycle", 
       "hydro_vu_pull", "final_output"),
  full.names = T)

flagged_2025_dataset <- flagged_2025_files %>% 
  map(\(file_path){
    site_parameter_df <- read_csv(file_path, show_col_types = F)
    return(site_parameter_df)
  }) %>%
  set_names(map_chr(., ~ paste0(unique(.x$site), "-", unique(.x$parameter)))) %>%
  keep_at(imap_lgl(., ~ grepl(site_level_string, .y))) %>%
  keep_at(imap_lgl(., ~ grepl(parameter_level_string, .y))) 


all_data_2025 <- flagged_2025_dataset %>%
  bind_rows() %>% 
  data.table() %>%
  filter( !is.na(site),
    # Filter based on DT. The 2025 data is in UTC
    DT_round >= as.POSIXct("2025-01-01 00:00:00", tz = "UTC") & DT_round <= as.POSIXct("2025-12-31 11:59:59", tz = "UTC")
  ) %>%
  #converting to MST for later use
  mutate(
    DT_round = with_tz(DT_round, tz = "MST"),
    DT_join = as.character(DT_round), 
    last_site_visit = with_tz(last_site_visit, "MST"), 
    mean = if_else(is.na(mal_flag), mean, NA) # remove reported sensor malfunctions
  )%>%
  dplyr::select(DT_round, DT_join, site, parameter, mean , auto_flag,mal_flag,  last_site_visit)
```


### Cleaning tasks (reducing overflagging, interpolation, smoothing, hourly median)
```{r}
all_data_2025_final <- all_data_2025%>%
  apply_cleaning_filters(df = ., new_value_col = "mean_cleaned")%>%
  apply_interpolation_missing_data(df = ., value_col = "mean_cleaned", new_value_col = "mean_filled", max_gap = 4, method = "spline")%>%
  apply_low_pass_binomial_filter(df = ., value_col = "mean_filled", new_value_col = "smoothed_mean")%>%
  apply_timestep_median(df = ., timestep  = "1 hour", value_col = "smoothed_mean", new_value_col = "hourly_median")
  
```

###  Save 2025 Data
```{r}

write_parquet(all_data_2025_final, paste0("data/upper_clp_dss/sensor/prepped/all_2025_sensor_data_", Sys.Date(),".parquet"))

rm(all_data_2025, flagged_2025_dataset, flagged_2025_files)
gc()
```

# Create long sensor dataset

```{r}

all_sensor_data <- bind_rows(all_data_2023_final, all_data_2024_final, all_data_2025_final)%>%
  select(-auto_flag, -mal_flag, -clean_flag, - flag )
  
write_parquet(all_sensor_data, paste0("data/upper_clp_dss/sensor/prepped/all_sensor_data_long", Sys.Date(), ".parquet"))
```

# Clean up space/memory
```{r}

#remove everything from memory
rm(list=ls())
gc()

```


# Create wide sensor dataset

This comes from the script above and should be updated when an API pull, QAQC is updated or as more data is verified.

### Get most recent files

```{r}

#Get most recent files
all_data_2023_wide_file <- list.files(path = "data/upper_clp_dss/sensor/prepped/", pattern = "all_2023_sensor_data_.*\\.parquet$", full.names = TRUE)%>%tail(1)
all_data_2024_wide_file <- list.files(path = "data/upper_clp_dss/sensor/prepped/", pattern = "all_2024_sensor_data_.*\\.parquet$", full.names = TRUE)%>%tail(1)
all_data_2025_wide_file <- list.files(path = "data/upper_clp_dss/sensor/prepped/", pattern = "all_2025_sensor_data_.*\\.parquet$", full.names = TRUE)%>%tail(1)
```

### Read in recent files

```{r}
# Read in all datasets, subset to hourly medians and then pivot to wide format
all_data <- list(read_parquet(file = all_data_2023_wide_file), 
                 read_parquet(file = all_data_2024_wide_file), 
                 read_parquet(file = all_data_2025_wide_file))

#combine each year's data
all_data_wide <- all_data %>%
  map(~ .x %>%
        rename(DT_hourly_round = DT_group) %>%
        distinct(site, DT_hourly_round, parameter, hourly_median) %>%
        pivot_wider(
          id_cols = c(site, DT_hourly_round),
          names_from = parameter,
          values_from = hourly_median
        )%>%
      #cleaning up turbidity values (rounding to nearest value, setting min to 1 and max to 1000)
      mutate(Turbidity = if_else(Turbidity <= 0, 0.1, if_else(Turbidity>=1000, 1000, Turbidity)))
      )
```


## Compute additional lagged parameters

```{r}
# custom function to calculate lagged parameters
source("src/calc_lagged_parameter.R")
#custom function to apply fdom temp correction
source("src/apply_fdom_temp_corr.R")

final_dataset <-all_data_wide%>%
  map(., function(df){
    df %>%
      #applying FDOM temperature correcting/standardization
      apply_fdom_temp_corr( fdom_col = "FDOM Fluorescence",
                            temp_col = "Temperature",
                            fdom_corr_col = "FDOM_corr_25c",
                            Tr = 25,
                            rho = -0.015)%>%
      # recent means (water temp)
      calc_lagged_parameter(site_col = "site", parameter_col = "Temperature",
                            dt_col = "DT_hourly_round", lag_fun = "mean",
                            time_period = "48 hour", new_value_col = "temp_mean_48hr")%>%
      calc_lagged_parameter(site_col = "site", parameter_col = "Temperature",
                            dt_col = "DT_hourly_round", lag_fun = "mean",
                            time_period = "24 hour", new_value_col = "temp_mean_24hr")%>%
      # recent maxes (temp, fdom)
      calc_lagged_parameter(site_col = "site", parameter_col = "FDOM Fluorescence",
                            dt_col = "DT_hourly_round", lag_fun = "max",
                            time_period = "6 hour", new_value_col = "fdom_max_6hr")%>%
      calc_lagged_parameter(site_col = "site", parameter_col = "FDOM_corr_25c",
                            dt_col = "DT_hourly_round", lag_fun = "max",
                            time_period = "6 hour", new_value_col = "fdom_corr_max_6hr")%>%
      calc_lagged_parameter(site_col = "site", parameter_col = "Temperature",
                            dt_col = "DT_hourly_round", lag_fun = "max",
                            time_period = "24 hour", new_value_col = "temp_max_24hr")%>%
      # recent variance/sd (sc)
      calc_lagged_parameter(site_col = "site", parameter_col = "Specific Conductivity",
                            dt_col = "DT_hourly_round", lag_fun = "var",
                            time_period = "24 hour", new_value_col = "sc_var_24hr")%>%
      mutate(sc_sd_24hr = sqrt(sc_var_24hr))%>%
      #12 hour median turbidity (proxy for recent storm event)
      calc_lagged_parameter(site_col = "site", parameter_col = "Turbidity",
                            dt_col = "DT_hourly_round", lag_fun = "median",
                            time_period = "24 hour", new_value_col = "turb_median_12hr")
  })%>%
  bind_rows()


```

## Get Flow data for CLP & join with sensor data

```{r}

min_date <- min(final_dataset$DT_hourly_round, na.rm = T)
max_date <- max(final_dataset$DT_hourly_round, na.rm = T)

canyon_mouth_flow <- get_telemetry_ts(abbrev = "CLAFTCCO", 
                 parameter = "DISCHRG", 
                 start_date = as.Date(min_date)- days(1),
                 end_date = as.Date(max_date)+ days(1),
                 timescale = "hour")%>%
  select(DT_hourly_round = meas_date, hourly_canyon_flow_cfs = meas_value)%>%
  mutate(DT_hourly_round = with_tz(ymd_hms(DT_hourly_round, tz = "America/Denver"), tzone = "MST"))

canyon_mouth_flow_daily <- get_telemetry_ts(abbrev = "CLAFTCCO", 
                 parameter = "DISCHRG", 
                 start_date = as.Date(min_date)- days(1),
                 end_date = as.Date(max_date)+ days(1),
                 timescale = "day")%>%
  select(date = datetime, daily_canyon_flow_cfs = meas_value)%>%
  mutate( date = as.Date(date))

final_dataset <- final_dataset%>%
  mutate(date = as_date(DT_hourly_round))%>%
  left_join(canyon_mouth_flow, by = "DT_hourly_round")%>%
  left_join(canyon_mouth_flow_daily, by = "date")%>%
  select(-date)
  

```

## Save wide dataset

```{r}
write_parquet(final_dataset, paste0("data/upper_clp_dss/sensor/prepped/all_sensor_data_wide_", Sys.Date(),".parquet"))

```

# Pull in & clean up grab sample data

## ROSS

```{r}

#discrete sample data and location metadata from most recent pub

source("src/pull_ROSS_zenodo_data.R")
zenodo_data <- pull_ROSS_zenodo_data(data_version = "v2025.07.01", DOI = "15883685", save_folder_dir = "data/upper_clp_dss/ross_clp_chem")

# Grab the water chemistry dataset

ross_water_chem <- zenodo_data[["most_recent_chem"]]%>%
  mutate(site_code = tolower(site_code), 
         #fixing site names based on sonde deployments with ROSS/Virridy sondes
         site_code = case_when(site_code == "archery" & DT_mst <= ymd("2024-11-30") ~ "archery_virridy",
                          site_code == "timberline" & DT_mst <= ymd("2024-11-30") ~ "riverbend_virridy",
                          site_code == "prospect" & DT_mst <= ymd("2024-11-30") ~ "cottonwood_virridy",
                          #updating to new names
                          site_code == "timberline" & DT_mst >= ymd("2024-11-30") ~ "riverbend",
                          site_code == "prospect" & DT_mst >= ymd("2024-11-30") ~ "cottonwood",
                          site_code == "lincoln" ~ "udall", 
                          site_code == "legacy" ~ "salyer",
                          site_code == "boxelder" ~ "elc",
                          site_code == "tamasag" ~ "bellvue",
                          T ~ site_code))%>%
  filter(DT_mst >= as_date("2023-09-01"))%>%
  distinct()%>%
  dplyr::select(site_code, 
         Date, 
         DT_sample = DT_mst,
         TOC, TN, DOC, ChlA, NO3, NH4, PO4, SC,Cl, lab_turb = Turbidity, TSS,DT_mst_char)%>%
  mutate(collector = "ROSS")
```

## Fort Collins UCLP Data

This is given to us by the City of Fort Collins Watershed Team (Diana Schmidt and Jared Heath). 
2025 Dataset is not yet finalized but can likely still be used for our first models


```{r}

FC_chem_2022_2024 <- read_csv("data/upper_clp_dss/fc_clp_chem/UCLP_CWQMP_database_2022-24.csv", show_col_types = F)%>%
  mutate(Date = as.POSIXct(Date, format = "%m/%d/%y"), 
         NO3 = NO3_N/ 0.2259, 
         SC = TDS/ 0.65, #converting TDS to SC
         Cl = as.numeric(Cl), 
         lab_turb = as.numeric(Turbidity),
         TN = as.numeric(TN_calc),
         ChlA = NA_integer_,
         DOC = NA_integer_, 
         NH4 = NA_integer_, 
         TSS = NA_integer_,
         site_code = tolower(ShortDesc))%>%
  #filter to the period of our sensor deployment and sites where we have sensors
  filter(site_code %in% c("chd", 'pbd', "sfm") & Date >= as_date("2023-09-01"))%>%
  #grab columns of interest
  dplyr::select(site_code, Date,
         TOC, TN, DOC, ChlA, NO3, NH4, PO4 = oPhos, SC,Cl, lab_turb, TSS)%>%
  mutate(collector = "FC")

FC_field_data_2022_2024 <- readxl::read_xlsx(path = "data/upper_clp_dss/fc_clp_chem/UCLP_field_data_2022-24.xlsx")%>%
  mutate(
    Date = as.Date(`Date (MM/DD/YYYY)`),
    # Combine date and time into one datetime column from MT to MST for consistency 
    dt_mst = with_tz(ymd_hms(paste(
      format(`Date (MM/DD/YYYY)`, "%Y-%m-%d"), 
      format(`Time (HH:MM:SS)`, "%H:%M:%S")
    ), tz = "America/Denver"), tzone = "MST"),
    # Remove numbers from site name
    site_code = tolower(gsub("\\d+", "", `Site Name`)), 
    DT_mst_char = as.character(dt_mst)
  )%>%
  dplyr::select(site_code, Date, DT_sample = dt_mst, DT_mst_char)
  
# Combine the chemistry data with the field data
FC_chem_2022_2024 <- FC_chem_2022_2024 %>%
  left_join(FC_field_data_2022_2024, by = c("site_code", "Date")) 


# Repeat for 2025 data (TOC only!)

FC_chem_2025 <- readxl::read_xlsx("data/upper_clp_dss/fc_clp_chem/UCLP_CWQMP_TOC_202504-202506.xlsx")%>%
  filter(grepl(x = `Parameter Name`, pattern = "Total Organic Carbon", ignore.case = TRUE), 
          `Entered Unit` == "mg/L",
         grepl(x = `Location Name`, pattern = "CHD|PBD|SFM|PBR", ignore.case = TRUE))%>%
  dplyr::select(
    Date = `Sampled Date`, 
    site_name = `Location Name`,
    value = `Corrected Result`
  )%>%
  mutate(site_code = case_when(
    grepl("PBR", site_name, ignore.case = TRUE) ~ "pbr",
    grepl("CHD", site_name, ignore.case = TRUE) ~ "chd",
    grepl("PBD", site_name, ignore.case = TRUE) ~ "pbd",
    grepl("SFM", site_name, ignore.case = TRUE) ~ "sfm",
    TRUE ~ NA_character_),
         TOC = as.numeric(value))%>%
  mutate(collector = "FC")%>%
  dplyr::select(site_code, Date, TOC, collector)

FC_field_data_2025 <- readxl::read_xlsx(path = "data/upper_clp_dss/fc_clp_chem/UCLP_field_data_04-062025.xlsx")%>%
  mutate(
    Date = as.Date(`Date (MM/DD/YYYY)`),
    # Combine date and time into one datetime column from MT to MST for consistency 
    dt_mst = with_tz(ymd_hms(paste(
      format(`Date (MM/DD/YYYY)`, "%Y-%m-%d"), 
      format(`Time (HH:MM:SS)`, "%H:%M:%S")
    ), tz = "America/Denver"), tzone = "MST"),
    # Remove numbers from site name
    site_code = tolower(gsub("\\d+", "", `Site Name`)), 
    DT_mst_char = as.character(dt_mst)
  )%>%
  dplyr::select(site_code, Date, DT_sample = dt_mst, DT_mst_char)


FC_chem_2025 <- FC_chem_2025 %>%
  left_join(FC_field_data_2025, by = c("site_code", "Date"))%>%
  na.omit()


FC_chem <- bind_rows(FC_chem_2022_2024, FC_chem_2025)


#write_csv(FC_chem, "data/FC_clp_chem/FC_testing_data.csv")
```

## Combine data
```{r}

water_chem <- bind_rows(ross_water_chem, FC_chem)

#save for later use
write_parquet(water_chem, "data/upper_clp_dss/modeling/ROSS_FC_water_chemistry.parquet")


# ggplot(water_chem%>%
#          filter(site_code %in% c("sfm", "chd", "pbr")), aes(x = Date, y = TOC, color = collector)) +
#   geom_point() +
#   geom_line()+
#   facet_wrap(~site_code, scales = "free_y") +
#   labs(title = "Water Chemistry Data from Ross and FC",
#        x = "Date",
#        y = "TOC (mg/L)") +
#   theme_minimal()

```

